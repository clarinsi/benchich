Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc', 'I-deriv-per']
(398681, 4) (51190, 4) (49764, 4)
     sentence_id      words labels   id
717          717      Kazna      O  717
718          718  medijskom      O  718
719          719     mogulu      O  719
720          720   obnovila      O  720
721          721   raspravu      O  721
Training started. Current model: csebert, no. of epochs: 4
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of bert model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 73.88 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.19366688451795463, 'precision': 0.7608637696614237, 'recall': 0.6694815857377434, 'f1_score': 0.7122535562765162}
Evaluation completed.
It took 6.22 minutes for 51190 instances.
Run 0 finished.
Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 615/615 [00:00<00:00, 114kB/s]
Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]Downloading model.safetensors:   1%|          | 10.5M/1.12G [00:00<00:19, 55.9MB/s]Downloading model.safetensors:   2%|▏         | 21.0M/1.12G [00:00<00:15, 68.7MB/s]Downloading model.safetensors:   3%|▎         | 31.5M/1.12G [00:00<00:25, 42.6MB/s]Downloading model.safetensors:   4%|▍         | 41.9M/1.12G [00:00<00:19, 54.9MB/s]Downloading model.safetensors:   5%|▍         | 52.4M/1.12G [00:00<00:16, 65.7MB/s]Downloading model.safetensors:   6%|▌         | 62.9M/1.12G [00:00<00:14, 73.5MB/s]Downloading model.safetensors:   7%|▋         | 73.4M/1.12G [00:01<00:13, 77.4MB/s]Downloading model.safetensors:   8%|▊         | 83.9M/1.12G [00:01<00:12, 79.4MB/s]Downloading model.safetensors:   8%|▊         | 94.4M/1.12G [00:01<00:12, 83.8MB/s]Downloading model.safetensors:   9%|▉         | 105M/1.12G [00:01<00:11, 86.7MB/s] Downloading model.safetensors:  10%|█         | 115M/1.12G [00:01<00:11, 86.7MB/s]Downloading model.safetensors:  11%|█▏        | 126M/1.12G [00:01<00:11, 86.4MB/s]Downloading model.safetensors:  12%|█▏        | 136M/1.12G [00:01<00:11, 88.3MB/s]Downloading model.safetensors:  13%|█▎        | 147M/1.12G [00:01<00:10, 91.0MB/s]Downloading model.safetensors:  14%|█▍        | 157M/1.12G [00:02<00:10, 89.9MB/s]Downloading model.safetensors:  15%|█▌        | 168M/1.12G [00:02<00:10, 93.5MB/s]Downloading model.safetensors:  16%|█▌        | 178M/1.12G [00:02<00:10, 93.3MB/s]Downloading model.safetensors:  17%|█▋        | 189M/1.12G [00:02<00:17, 54.5MB/s]Downloading model.safetensors:  18%|█▊        | 199M/1.12G [00:02<00:14, 62.9MB/s]Downloading model.safetensors:  19%|█▉        | 210M/1.12G [00:02<00:13, 69.3MB/s]Downloading model.safetensors:  20%|█▉        | 220M/1.12G [00:02<00:11, 75.3MB/s]Downloading model.safetensors:  21%|██        | 231M/1.12G [00:03<00:10, 80.8MB/s]Downloading model.safetensors:  22%|██▏       | 241M/1.12G [00:03<00:17, 51.3MB/s]Downloading model.safetensors:  23%|██▎       | 252M/1.12G [00:03<00:14, 59.6MB/s]Downloading model.safetensors:  23%|██▎       | 262M/1.12G [00:03<00:12, 66.3MB/s]Downloading model.safetensors:  24%|██▍       | 273M/1.12G [00:03<00:11, 74.1MB/s]Downloading model.safetensors:  25%|██▌       | 283M/1.12G [00:03<00:10, 79.4MB/s]Downloading model.safetensors:  26%|██▋       | 294M/1.12G [00:04<00:09, 83.4MB/s]Downloading model.safetensors:  27%|██▋       | 304M/1.12G [00:04<00:09, 86.1MB/s]Downloading model.safetensors:  28%|██▊       | 315M/1.12G [00:04<00:09, 86.8MB/s]Downloading model.safetensors:  29%|██▉       | 325M/1.12G [00:04<00:09, 87.6MB/s]Downloading model.safetensors:  30%|███       | 336M/1.12G [00:04<00:08, 89.5MB/s]Downloading model.safetensors:  31%|███       | 346M/1.12G [00:04<00:08, 87.9MB/s]Downloading model.safetensors:  32%|███▏      | 357M/1.12G [00:04<00:08, 89.5MB/s]Downloading model.safetensors:  33%|███▎      | 367M/1.12G [00:04<00:08, 88.9MB/s]Downloading model.safetensors:  34%|███▍      | 377M/1.12G [00:04<00:08, 91.9MB/s]Downloading model.safetensors:  35%|███▍      | 388M/1.12G [00:05<00:07, 94.2MB/s]Downloading model.safetensors:  36%|███▌      | 398M/1.12G [00:05<00:07, 91.0MB/s]Downloading model.safetensors:  37%|███▋      | 409M/1.12G [00:05<00:07, 92.6MB/s]Downloading model.safetensors:  38%|███▊      | 419M/1.12G [00:05<00:13, 53.3MB/s]Downloading model.safetensors:  39%|███▊      | 430M/1.12G [00:05<00:11, 61.8MB/s]Downloading model.safetensors:  39%|███▉      | 440M/1.12G [00:05<00:10, 67.3MB/s]Downloading model.safetensors:  40%|████      | 451M/1.12G [00:05<00:09, 73.8MB/s]Downloading model.safetensors:  41%|████▏     | 461M/1.12G [00:06<00:08, 78.9MB/s]Downloading model.safetensors:  42%|████▏     | 472M/1.12G [00:06<00:07, 84.2MB/s]Downloading model.safetensors:  43%|████▎     | 482M/1.12G [00:06<00:07, 86.1MB/s]Downloading model.safetensors:  44%|████▍     | 493M/1.12G [00:06<00:07, 85.1MB/s]Downloading model.safetensors:  45%|████▌     | 503M/1.12G [00:06<00:11, 52.2MB/s]Downloading model.safetensors:  46%|████▌     | 514M/1.12G [00:06<00:09, 60.2MB/s]Downloading model.safetensors:  47%|████▋     | 524M/1.12G [00:07<00:09, 65.0MB/s]Downloading model.safetensors:  48%|████▊     | 535M/1.12G [00:07<00:08, 72.0MB/s]Downloading model.safetensors:  49%|████▉     | 545M/1.12G [00:07<00:07, 75.9MB/s]Downloading model.safetensors:  50%|████▉     | 556M/1.12G [00:07<00:07, 79.4MB/s]Downloading model.safetensors:  51%|█████     | 566M/1.12G [00:07<00:06, 78.9MB/s]Downloading model.safetensors:  52%|█████▏    | 577M/1.12G [00:07<00:06, 84.4MB/s]Downloading model.safetensors:  53%|█████▎    | 587M/1.12G [00:08<00:09, 52.9MB/s]Downloading model.safetensors:  54%|█████▎    | 598M/1.12G [00:08<00:08, 61.0MB/s]Downloading model.safetensors:  55%|█████▍    | 608M/1.12G [00:08<00:07, 68.7MB/s]Downloading model.safetensors:  55%|█████▌    | 619M/1.12G [00:08<00:06, 73.7MB/s]Downloading model.safetensors:  56%|█████▋    | 629M/1.12G [00:08<00:06, 78.0MB/s]Downloading model.safetensors:  57%|█████▋    | 640M/1.12G [00:08<00:05, 82.6MB/s]Downloading model.safetensors:  58%|█████▊    | 650M/1.12G [00:08<00:05, 84.2MB/s]Downloading model.safetensors:  59%|█████▉    | 661M/1.12G [00:08<00:05, 87.7MB/s]Downloading model.safetensors:  60%|██████    | 671M/1.12G [00:08<00:04, 90.4MB/s]Downloading model.safetensors:  61%|██████    | 682M/1.12G [00:09<00:04, 93.9MB/s]Downloading model.safetensors:  62%|██████▏   | 692M/1.12G [00:09<00:07, 54.4MB/s]Downloading model.safetensors:  63%|██████▎   | 703M/1.12G [00:09<00:06, 62.9MB/s]Downloading model.safetensors:  64%|██████▍   | 713M/1.12G [00:09<00:05, 71.2MB/s]Downloading model.safetensors:  65%|██████▍   | 724M/1.12G [00:09<00:05, 76.5MB/s]Downloading model.safetensors:  66%|██████▌   | 734M/1.12G [00:09<00:04, 82.1MB/s]Downloading model.safetensors:  67%|██████▋   | 744M/1.12G [00:09<00:04, 84.3MB/s]Downloading model.safetensors:  68%|██████▊   | 755M/1.12G [00:10<00:08, 42.0MB/s]Downloading model.safetensors:  69%|██████▊   | 765M/1.12G [00:10<00:09, 36.2MB/s]Downloading model.safetensors:  70%|██████▉   | 776M/1.12G [00:10<00:07, 44.5MB/s]Downloading model.safetensors:  70%|███████   | 786M/1.12G [00:11<00:06, 52.3MB/s]Downloading model.safetensors:  71%|███████▏  | 797M/1.12G [00:11<00:05, 60.4MB/s]Downloading model.safetensors:  72%|███████▏  | 807M/1.12G [00:11<00:04, 66.5MB/s]Downloading model.safetensors:  73%|███████▎  | 818M/1.12G [00:11<00:06, 47.2MB/s]Downloading model.safetensors:  74%|███████▍  | 828M/1.12G [00:11<00:05, 55.4MB/s]Downloading model.safetensors:  75%|███████▌  | 839M/1.12G [00:11<00:04, 63.1MB/s]Downloading model.safetensors:  76%|███████▌  | 849M/1.12G [00:12<00:03, 71.0MB/s]Downloading model.safetensors:  77%|███████▋  | 860M/1.12G [00:12<00:03, 75.5MB/s]Downloading model.safetensors:  78%|███████▊  | 870M/1.12G [00:12<00:03, 78.2MB/s]Downloading model.safetensors:  79%|███████▉  | 881M/1.12G [00:12<00:02, 83.4MB/s]Downloading model.safetensors:  80%|███████▉  | 891M/1.12G [00:12<00:02, 87.3MB/s]Downloading model.safetensors:  81%|████████  | 902M/1.12G [00:12<00:02, 89.3MB/s]Downloading model.safetensors:  82%|████████▏ | 912M/1.12G [00:12<00:02, 90.9MB/s]Downloading model.safetensors:  83%|████████▎ | 923M/1.12G [00:12<00:02, 91.4MB/s]Downloading model.safetensors:  84%|████████▎ | 933M/1.12G [00:13<00:03, 53.8MB/s]Downloading model.safetensors:  85%|████████▍ | 944M/1.12G [00:13<00:02, 62.1MB/s]Downloading model.safetensors:  86%|████████▌ | 954M/1.12G [00:13<00:03, 45.3MB/s]Downloading model.safetensors:  86%|████████▋ | 965M/1.12G [00:13<00:02, 53.1MB/s]Downloading model.safetensors:  87%|████████▋ | 975M/1.12G [00:14<00:03, 41.6MB/s]Downloading model.safetensors:  88%|████████▊ | 986M/1.12G [00:14<00:02, 49.8MB/s]Downloading model.safetensors:  89%|████████▉ | 996M/1.12G [00:14<00:02, 57.9MB/s]Downloading model.safetensors:  90%|█████████ | 1.01G/1.12G [00:14<00:01, 64.6MB/s]Downloading model.safetensors:  91%|█████████ | 1.02G/1.12G [00:14<00:01, 71.9MB/s]Downloading model.safetensors:  92%|█████████▏| 1.03G/1.12G [00:15<00:01, 48.4MB/s]Downloading model.safetensors:  93%|█████████▎| 1.04G/1.12G [00:15<00:01, 57.0MB/s]Downloading model.safetensors:  94%|█████████▍| 1.05G/1.12G [00:15<00:01, 65.0MB/s]Downloading model.safetensors:  95%|█████████▍| 1.06G/1.12G [00:15<00:00, 71.7MB/s]Downloading model.safetensors:  96%|█████████▌| 1.07G/1.12G [00:15<00:00, 76.3MB/s]Downloading model.safetensors:  97%|█████████▋| 1.08G/1.12G [00:15<00:00, 79.0MB/s]Downloading model.safetensors:  98%|█████████▊| 1.09G/1.12G [00:15<00:00, 82.0MB/s]Downloading model.safetensors:  99%|█████████▊| 1.10G/1.12G [00:15<00:00, 85.7MB/s]Downloading model.safetensors: 100%|█████████▉| 1.11G/1.12G [00:15<00:00, 88.1MB/s]Downloading model.safetensors: 100%|██████████| 1.12G/1.12G [00:15<00:00, 69.8MB/s]
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 7.05MB/s]Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 7.00MB/s]
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training started. Current model: xlm-r-base, no. of epochs: 5
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 105.75 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.20378424972412196, 'precision': 0.7276995305164319, 'recall': 0.6181093126905934, 'f1_score': 0.6684424150177574}
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 6.22 minutes for 51190 instances.
Run 0 finished.
Training started. Current model: xlm-r-large, no. of epochs: 7
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 382.73 minutes for 398681 instances.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.4712727679780059, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Evaluation completed.
It took 8.08 minutes for 51190 instances.
Run 0 finished.
Downloading config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 467/467 [00:00<00:00, 391kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]Downloading pytorch_model.bin:   2%|▏         | 10.5M/443M [00:00<00:37, 11.5MB/s]Downloading pytorch_model.bin:   5%|▍         | 21.0M/443M [00:01<00:23, 18.3MB/s]Downloading pytorch_model.bin:   7%|▋         | 31.5M/443M [00:01<00:19, 21.3MB/s]Downloading pytorch_model.bin:   9%|▉         | 41.9M/443M [00:01<00:16, 24.0MB/s]Downloading pytorch_model.bin:  12%|█▏        | 52.4M/443M [00:02<00:14, 27.3MB/s]Downloading pytorch_model.bin:  14%|█▍        | 62.9M/443M [00:02<00:13, 28.9MB/s]Downloading pytorch_model.bin:  17%|█▋        | 73.4M/443M [00:02<00:12, 29.2MB/s]Downloading pytorch_model.bin:  19%|█▉        | 83.9M/443M [00:03<00:12, 28.9MB/s]Downloading pytorch_model.bin:  21%|██▏       | 94.4M/443M [00:03<00:12, 28.2MB/s]Downloading pytorch_model.bin:  24%|██▎       | 105M/443M [00:04<00:11, 28.3MB/s] Downloading pytorch_model.bin:  26%|██▌       | 115M/443M [00:04<00:11, 29.2MB/s]Downloading pytorch_model.bin:  28%|██▊       | 126M/443M [00:04<00:10, 30.0MB/s]Downloading pytorch_model.bin:  31%|███       | 136M/443M [00:05<00:10, 30.0MB/s]Downloading pytorch_model.bin:  33%|███▎      | 147M/443M [00:05<00:10, 28.6MB/s]Downloading pytorch_model.bin:  36%|███▌      | 157M/443M [00:05<00:09, 28.6MB/s]Downloading pytorch_model.bin:  38%|███▊      | 168M/443M [00:06<00:09, 28.7MB/s]Downloading pytorch_model.bin:  40%|████      | 178M/443M [00:06<00:09, 26.6MB/s]Downloading pytorch_model.bin:  43%|████▎     | 189M/443M [00:07<00:09, 26.3MB/s]Downloading pytorch_model.bin:  45%|████▌     | 199M/443M [00:07<00:08, 27.6MB/s]Downloading pytorch_model.bin:  47%|████▋     | 210M/443M [00:07<00:07, 29.4MB/s]Downloading pytorch_model.bin:  50%|████▉     | 220M/443M [00:08<00:07, 29.6MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 231M/443M [00:08<00:06, 31.1MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 241M/443M [00:08<00:06, 30.7MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 252M/443M [00:09<00:06, 29.2MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 262M/443M [00:09<00:06, 29.3MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 273M/443M [00:09<00:05, 29.6MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 283M/443M [00:10<00:05, 29.8MB/s]Downloading pytorch_model.bin:  66%|██████▋   | 294M/443M [00:10<00:05, 29.5MB/s]Downloading pytorch_model.bin:  69%|██████▊   | 304M/443M [00:10<00:04, 28.3MB/s]Downloading pytorch_model.bin:  71%|███████   | 315M/443M [00:11<00:04, 27.4MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 325M/443M [00:11<00:04, 28.0MB/s]Downloading pytorch_model.bin:  76%|███████▌  | 336M/443M [00:12<00:03, 27.3MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 346M/443M [00:12<00:03, 27.2MB/s]Downloading pytorch_model.bin:  81%|████████  | 357M/443M [00:12<00:03, 26.6MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 367M/443M [00:13<00:02, 26.6MB/s]Downloading pytorch_model.bin:  85%|████████▌ | 377M/443M [00:13<00:02, 26.8MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 388M/443M [00:14<00:01, 27.3MB/s]Downloading pytorch_model.bin:  90%|█████████ | 398M/443M [00:14<00:01, 27.4MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 409M/443M [00:14<00:01, 27.3MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 419M/443M [00:15<00:00, 27.1MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 430M/443M [00:15<00:00, 26.8MB/s]Downloading pytorch_model.bin: 100%|█████████▉| 440M/443M [00:16<00:00, 26.7MB/s]Downloading pytorch_model.bin: 100%|██████████| 443M/443M [00:16<00:00, 27.4MB/s]
Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']
- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading vocab.txt:   0%|          | 0.00/231k [00:00<?, ?B/s]Downloading vocab.txt: 100%|██████████| 231k/231k [00:00<00:00, 1.15MB/s]Downloading vocab.txt: 100%|██████████| 231k/231k [00:00<00:00, 1.14MB/s]
Downloading tokenizer_config.json:   0%|          | 0.00/83.0 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|██████████| 83.0/83.0 [00:00<00:00, 114kB/s]
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training started. Current model: bertic, no. of epochs: 9
INFO:simpletransformers.ner.ner_model: Training of electra model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 162.19 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.23131998961804387, 'precision': 0.747148288973384, 'recall': 0.645320197044335, 'f1_score': 0.6925110132158591}
Evaluation completed.
It took 5.63 minutes for 51190 instances.
Run 0 finished.
Downloading config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 714/714 [00:00<00:00, 690kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|          | 10.5M/2.24G [00:00<02:36, 14.2MB/s]Downloading pytorch_model.bin:   1%|          | 21.0M/2.24G [00:00<01:26, 25.6MB/s]Downloading pytorch_model.bin:   1%|▏         | 31.5M/2.24G [00:01<01:04, 34.4MB/s]Downloading pytorch_model.bin:   2%|▏         | 41.9M/2.24G [00:01<00:53, 41.0MB/s]Downloading pytorch_model.bin:   2%|▏         | 52.4M/2.24G [00:01<00:47, 45.7MB/s]Downloading pytorch_model.bin:   3%|▎         | 62.9M/2.24G [00:01<00:44, 48.6MB/s]Downloading pytorch_model.bin:   3%|▎         | 73.4M/2.24G [00:01<00:42, 51.1MB/s]Downloading pytorch_model.bin:   4%|▎         | 83.9M/2.24G [00:02<00:40, 53.2MB/s]Downloading pytorch_model.bin:   4%|▍         | 94.4M/2.24G [00:02<00:39, 54.5MB/s]Downloading pytorch_model.bin:   5%|▍         | 105M/2.24G [00:02<00:38, 55.5MB/s] Downloading pytorch_model.bin:   5%|▌         | 115M/2.24G [00:02<00:37, 56.2MB/s]Downloading pytorch_model.bin:   6%|▌         | 126M/2.24G [00:02<00:37, 56.6MB/s]Downloading pytorch_model.bin:   6%|▌         | 136M/2.24G [00:02<00:36, 56.9MB/s]Downloading pytorch_model.bin:   7%|▋         | 147M/2.24G [00:03<00:36, 57.2MB/s]Downloading pytorch_model.bin:   7%|▋         | 157M/2.24G [00:03<00:36, 57.2MB/s]Downloading pytorch_model.bin:   7%|▋         | 168M/2.24G [00:03<00:36, 57.4MB/s]Downloading pytorch_model.bin:   8%|▊         | 178M/2.24G [00:03<00:36, 57.1MB/s]Downloading pytorch_model.bin:   8%|▊         | 189M/2.24G [00:03<00:35, 57.9MB/s]Downloading pytorch_model.bin:   9%|▉         | 199M/2.24G [00:04<00:35, 57.6MB/s]Downloading pytorch_model.bin:   9%|▉         | 210M/2.24G [00:04<00:35, 57.2MB/s]Downloading pytorch_model.bin:  10%|▉         | 220M/2.24G [00:04<00:35, 57.4MB/s]Downloading pytorch_model.bin:  10%|█         | 231M/2.24G [00:04<00:35, 57.0MB/s]Downloading pytorch_model.bin:  11%|█         | 241M/2.24G [00:04<00:34, 57.6MB/s]Downloading pytorch_model.bin:  11%|█         | 252M/2.24G [00:04<00:34, 57.7MB/s]Downloading pytorch_model.bin:  12%|█▏        | 262M/2.24G [00:05<00:34, 57.7MB/s]Downloading pytorch_model.bin:  12%|█▏        | 273M/2.24G [00:05<00:34, 57.7MB/s]Downloading pytorch_model.bin:  13%|█▎        | 283M/2.24G [00:05<00:34, 57.1MB/s]Downloading pytorch_model.bin:  13%|█▎        | 294M/2.24G [00:05<00:33, 57.6MB/s]Downloading pytorch_model.bin:  14%|█▎        | 304M/2.24G [00:05<00:33, 57.3MB/s]Downloading pytorch_model.bin:  14%|█▍        | 315M/2.24G [00:06<00:33, 58.1MB/s]Downloading pytorch_model.bin:  15%|█▍        | 325M/2.24G [00:06<00:33, 56.4MB/s]Downloading pytorch_model.bin:  15%|█▍        | 336M/2.24G [00:06<00:32, 58.3MB/s]Downloading pytorch_model.bin:  15%|█▌        | 346M/2.24G [00:06<00:32, 58.2MB/s]Downloading pytorch_model.bin:  16%|█▌        | 357M/2.24G [00:06<00:32, 58.0MB/s]Downloading pytorch_model.bin:  16%|█▋        | 367M/2.24G [00:06<00:32, 57.9MB/s]Downloading pytorch_model.bin:  17%|█▋        | 377M/2.24G [00:07<00:32, 57.5MB/s]Downloading pytorch_model.bin:  17%|█▋        | 388M/2.24G [00:07<00:32, 57.8MB/s]Downloading pytorch_model.bin:  18%|█▊        | 398M/2.24G [00:07<00:31, 57.8MB/s]Downloading pytorch_model.bin:  18%|█▊        | 409M/2.24G [00:07<00:31, 57.7MB/s]Downloading pytorch_model.bin:  19%|█▊        | 419M/2.24G [00:08<00:44, 40.6MB/s]Downloading pytorch_model.bin:  19%|█▉        | 430M/2.24G [00:08<00:38, 47.6MB/s]Downloading pytorch_model.bin:  20%|█▉        | 440M/2.24G [00:08<00:32, 56.1MB/s]Downloading pytorch_model.bin:  20%|██        | 451M/2.24G [00:08<00:27, 64.8MB/s]Downloading pytorch_model.bin:  21%|██        | 461M/2.24G [00:08<00:25, 69.2MB/s]Downloading pytorch_model.bin:  21%|██        | 472M/2.24G [00:08<00:27, 65.3MB/s]Downloading pytorch_model.bin:  22%|██▏       | 482M/2.24G [00:08<00:28, 62.6MB/s]Downloading pytorch_model.bin:  22%|██▏       | 493M/2.24G [00:09<00:28, 61.0MB/s]Downloading pytorch_model.bin:  22%|██▏       | 503M/2.24G [00:09<00:29, 58.9MB/s]Downloading pytorch_model.bin:  23%|██▎       | 514M/2.24G [00:09<00:29, 58.1MB/s]Downloading pytorch_model.bin:  23%|██▎       | 524M/2.24G [00:09<00:29, 57.3MB/s]Downloading pytorch_model.bin:  24%|██▍       | 535M/2.24G [00:09<00:30, 56.9MB/s]Downloading pytorch_model.bin:  24%|██▍       | 545M/2.24G [00:10<00:29, 57.1MB/s]Downloading pytorch_model.bin:  25%|██▍       | 556M/2.24G [00:10<00:32, 51.3MB/s]Downloading pytorch_model.bin:  25%|██▌       | 566M/2.24G [00:10<00:30, 54.6MB/s]Downloading pytorch_model.bin:  26%|██▌       | 577M/2.24G [00:10<00:30, 54.3MB/s]Downloading pytorch_model.bin:  26%|██▌       | 587M/2.24G [00:10<00:28, 58.4MB/s]Downloading pytorch_model.bin:  27%|██▋       | 598M/2.24G [00:10<00:26, 61.0MB/s]Downloading pytorch_model.bin:  27%|██▋       | 608M/2.24G [00:11<00:27, 60.0MB/s]Downloading pytorch_model.bin:  28%|██▊       | 619M/2.24G [00:11<00:27, 59.2MB/s]Downloading pytorch_model.bin:  28%|██▊       | 629M/2.24G [00:11<00:27, 58.7MB/s]Downloading pytorch_model.bin:  29%|██▊       | 640M/2.24G [00:11<00:27, 58.4MB/s]Downloading pytorch_model.bin:  29%|██▉       | 650M/2.24G [00:11<00:27, 58.2MB/s]Downloading pytorch_model.bin:  29%|██▉       | 661M/2.24G [00:12<00:27, 58.0MB/s]Downloading pytorch_model.bin:  30%|██▉       | 671M/2.24G [00:12<00:27, 57.9MB/s]Downloading pytorch_model.bin:  30%|███       | 682M/2.24G [00:12<00:26, 57.8MB/s]Downloading pytorch_model.bin:  31%|███       | 692M/2.24G [00:12<00:26, 57.7MB/s]Downloading pytorch_model.bin:  31%|███▏      | 703M/2.24G [00:12<00:26, 57.0MB/s]Downloading pytorch_model.bin:  32%|███▏      | 713M/2.24G [00:12<00:26, 58.0MB/s]Downloading pytorch_model.bin:  32%|███▏      | 724M/2.24G [00:13<00:26, 57.2MB/s]Downloading pytorch_model.bin:  33%|███▎      | 734M/2.24G [00:13<00:25, 58.1MB/s]Downloading pytorch_model.bin:  33%|███▎      | 744M/2.24G [00:13<00:25, 58.0MB/s]Downloading pytorch_model.bin:  34%|███▎      | 755M/2.24G [00:13<00:25, 57.2MB/s]Downloading pytorch_model.bin:  34%|███▍      | 765M/2.24G [00:13<00:25, 57.2MB/s]Downloading pytorch_model.bin:  35%|███▍      | 776M/2.24G [00:14<00:25, 57.5MB/s]Downloading pytorch_model.bin:  35%|███▌      | 786M/2.24G [00:14<00:25, 57.5MB/s]Downloading pytorch_model.bin:  36%|███▌      | 797M/2.24G [00:14<00:25, 57.5MB/s]Downloading pytorch_model.bin:  36%|███▌      | 807M/2.24G [00:14<00:24, 57.6MB/s]Downloading pytorch_model.bin:  37%|███▋      | 818M/2.24G [00:14<00:24, 57.7MB/s]Downloading pytorch_model.bin:  37%|███▋      | 828M/2.24G [00:14<00:24, 57.6MB/s]Downloading pytorch_model.bin:  37%|███▋      | 839M/2.24G [00:15<00:24, 57.7MB/s]Downloading pytorch_model.bin:  38%|███▊      | 849M/2.24G [00:15<00:24, 57.7MB/s]Downloading pytorch_model.bin:  38%|███▊      | 860M/2.24G [00:15<00:23, 57.7MB/s]Downloading pytorch_model.bin:  39%|███▉      | 870M/2.24G [00:15<00:23, 57.6MB/s]Downloading pytorch_model.bin:  39%|███▉      | 881M/2.24G [00:15<00:23, 57.7MB/s]Downloading pytorch_model.bin:  40%|███▉      | 891M/2.24G [00:16<00:23, 57.6MB/s]Downloading pytorch_model.bin:  40%|████      | 902M/2.24G [00:16<00:23, 57.7MB/s]Downloading pytorch_model.bin:  41%|████      | 912M/2.24G [00:16<00:22, 57.8MB/s]Downloading pytorch_model.bin:  41%|████      | 923M/2.24G [00:16<00:22, 57.7MB/s]Downloading pytorch_model.bin:  42%|████▏     | 933M/2.24G [00:16<00:22, 57.5MB/s]Downloading pytorch_model.bin:  42%|████▏     | 944M/2.24G [00:16<00:22, 57.7MB/s]Downloading pytorch_model.bin:  43%|████▎     | 954M/2.24G [00:17<00:22, 57.7MB/s]Downloading pytorch_model.bin:  43%|████▎     | 965M/2.24G [00:17<00:22, 56.8MB/s]Downloading pytorch_model.bin:  44%|████▎     | 975M/2.24G [00:17<00:21, 58.1MB/s]Downloading pytorch_model.bin:  44%|████▍     | 986M/2.24G [00:17<00:21, 57.9MB/s]Downloading pytorch_model.bin:  44%|████▍     | 996M/2.24G [00:17<00:21, 58.0MB/s]Downloading pytorch_model.bin:  45%|████▍     | 1.01G/2.24G [00:18<00:21, 57.7MB/s]Downloading pytorch_model.bin:  45%|████▌     | 1.02G/2.24G [00:18<00:21, 57.5MB/s]Downloading pytorch_model.bin:  46%|████▌     | 1.03G/2.24G [00:18<00:21, 57.4MB/s]Downloading pytorch_model.bin:  46%|████▋     | 1.04G/2.24G [00:18<00:20, 58.2MB/s]Downloading pytorch_model.bin:  47%|████▋     | 1.05G/2.24G [00:18<00:21, 55.9MB/s]Downloading pytorch_model.bin:  47%|████▋     | 1.06G/2.24G [00:19<00:22, 53.7MB/s]Downloading pytorch_model.bin:  48%|████▊     | 1.07G/2.24G [00:19<00:20, 56.0MB/s]Downloading pytorch_model.bin:  48%|████▊     | 1.08G/2.24G [00:19<00:19, 59.3MB/s]Downloading pytorch_model.bin:  49%|████▊     | 1.09G/2.24G [00:19<00:19, 59.2MB/s]Downloading pytorch_model.bin:  49%|████▉     | 1.10G/2.24G [00:19<00:19, 59.4MB/s]Downloading pytorch_model.bin:  50%|████▉     | 1.11G/2.24G [00:19<00:19, 58.7MB/s]Downloading pytorch_model.bin:  50%|█████     | 1.12G/2.24G [00:20<00:19, 58.2MB/s]Downloading pytorch_model.bin:  51%|█████     | 1.13G/2.24G [00:20<00:19, 57.9MB/s]Downloading pytorch_model.bin:  51%|█████     | 1.14G/2.24G [00:20<00:19, 57.7MB/s]Downloading pytorch_model.bin:  51%|█████▏    | 1.15G/2.24G [00:20<00:18, 57.3MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 1.16G/2.24G [00:20<00:23, 45.2MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 1.17G/2.24G [00:21<00:19, 53.5MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 1.18G/2.24G [00:21<00:17, 61.3MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 1.20G/2.24G [00:21<00:17, 58.1MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 1.21G/2.24G [00:21<00:16, 61.0MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 1.22G/2.24G [00:21<00:18, 56.4MB/s]Downloading pytorch_model.bin:  55%|█████▍    | 1.23G/2.24G [00:21<00:16, 59.9MB/s]Downloading pytorch_model.bin:  55%|█████▌    | 1.24G/2.24G [00:22<00:16, 59.3MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 1.25G/2.24G [00:22<00:16, 58.4MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 1.26G/2.24G [00:22<00:16, 58.2MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 1.27G/2.24G [00:22<00:16, 57.9MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 1.28G/2.24G [00:22<00:16, 58.0MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.29G/2.24G [00:22<00:16, 57.8MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.30G/2.24G [00:23<00:16, 57.8MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.31G/2.24G [00:23<00:16, 57.7MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 1.32G/2.24G [00:23<00:15, 57.7MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 1.33G/2.24G [00:23<00:15, 57.7MB/s]Downloading pytorch_model.bin:  60%|█████▉    | 1.34G/2.24G [00:23<00:15, 56.8MB/s]Downloading pytorch_model.bin:  60%|██████    | 1.35G/2.24G [00:24<00:16, 54.9MB/s]Downloading pytorch_model.bin:  61%|██████    | 1.36G/2.24G [00:24<00:16, 54.5MB/s]Downloading pytorch_model.bin:  61%|██████▏   | 1.37G/2.24G [00:24<00:15, 54.5MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 1.38G/2.24G [00:24<00:15, 56.5MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 1.39G/2.24G [00:24<00:14, 60.4MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 1.41G/2.24G [00:25<00:14, 59.6MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 1.42G/2.24G [00:25<00:14, 57.7MB/s]Downloading pytorch_model.bin:  64%|██████▎   | 1.43G/2.24G [00:25<00:13, 58.8MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 1.44G/2.24G [00:25<00:13, 58.4MB/s]Downloading pytorch_model.bin:  65%|██████▍   | 1.45G/2.24G [00:25<00:13, 58.4MB/s]Downloading pytorch_model.bin:  65%|██████▌   | 1.46G/2.24G [00:25<00:13, 57.5MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 1.47G/2.24G [00:26<00:13, 57.8MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 1.48G/2.24G [00:26<00:13, 58.2MB/s]Downloading pytorch_model.bin:  66%|██████▋   | 1.49G/2.24G [00:26<00:13, 57.4MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 1.50G/2.24G [00:26<00:12, 57.8MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 1.51G/2.24G [00:26<00:12, 57.1MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 1.52G/2.24G [00:27<00:12, 57.4MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 1.53G/2.24G [00:27<00:12, 57.8MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 1.54G/2.24G [00:27<00:12, 57.2MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 1.55G/2.24G [00:27<00:11, 57.9MB/s]Downloading pytorch_model.bin:  70%|██████▉   | 1.56G/2.24G [00:27<00:11, 57.8MB/s]Downloading pytorch_model.bin:  70%|███████   | 1.57G/2.24G [00:27<00:11, 57.5MB/s]Downloading pytorch_model.bin:  71%|███████   | 1.58G/2.24G [00:28<00:11, 57.6MB/s]Downloading pytorch_model.bin:  71%|███████   | 1.59G/2.24G [00:28<00:11, 57.3MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.60G/2.24G [00:28<00:15, 40.7MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.61G/2.24G [00:28<00:13, 47.3MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.63G/2.24G [00:28<00:11, 54.4MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.64G/2.24G [00:29<00:09, 61.0MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.65G/2.24G [00:29<00:08, 67.1MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 1.66G/2.24G [00:29<00:08, 67.5MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 1.67G/2.24G [00:29<00:08, 64.1MB/s]Downloading pytorch_model.bin:  75%|███████▍  | 1.68G/2.24G [00:29<00:09, 62.2MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 1.69G/2.24G [00:29<00:09, 60.0MB/s]Downloading pytorch_model.bin:  76%|███████▌  | 1.70G/2.24G [00:30<00:09, 60.0MB/s]Downloading pytorch_model.bin:  76%|███████▋  | 1.71G/2.24G [00:30<00:09, 58.8MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.72G/2.24G [00:30<00:08, 58.4MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.73G/2.24G [00:30<00:08, 57.5MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 1.74G/2.24G [00:30<00:08, 58.2MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 1.75G/2.24G [00:31<00:11, 43.6MB/s]Downloading pytorch_model.bin:  79%|███████▊  | 1.76G/2.24G [00:31<00:09, 48.6MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 1.77G/2.24G [00:31<00:08, 56.8MB/s]Downloading pytorch_model.bin:  80%|███████▉  | 1.78G/2.24G [00:31<00:07, 65.3MB/s]Downloading pytorch_model.bin:  80%|████████  | 1.79G/2.24G [00:31<00:06, 65.9MB/s]Downloading pytorch_model.bin:  80%|████████  | 1.80G/2.24G [00:31<00:06, 63.3MB/s]Downloading pytorch_model.bin:  81%|████████  | 1.81G/2.24G [00:32<00:07, 57.5MB/s]Downloading pytorch_model.bin:  81%|████████▏ | 1.82G/2.24G [00:32<00:07, 59.4MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 1.84G/2.24G [00:32<00:07, 57.6MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 1.85G/2.24G [00:32<00:06, 60.1MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 1.86G/2.24G [00:32<00:06, 59.6MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 1.87G/2.24G [00:33<00:06, 58.9MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 1.88G/2.24G [00:33<00:06, 58.2MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 1.89G/2.24G [00:33<00:06, 58.1MB/s]Downloading pytorch_model.bin:  85%|████████▍ | 1.90G/2.24G [00:33<00:05, 57.8MB/s]Downloading pytorch_model.bin:  85%|████████▌ | 1.91G/2.24G [00:33<00:05, 57.5MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 1.92G/2.24G [00:33<00:05, 56.4MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 1.93G/2.24G [00:34<00:05, 57.6MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 1.94G/2.24G [00:34<00:05, 57.6MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 1.95G/2.24G [00:34<00:05, 57.4MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 1.96G/2.24G [00:34<00:04, 57.4MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 1.97G/2.24G [00:34<00:04, 57.4MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 1.98G/2.24G [00:35<00:04, 57.6MB/s]Downloading pytorch_model.bin:  89%|████████▉ | 1.99G/2.24G [00:35<00:05, 41.9MB/s]Downloading pytorch_model.bin:  89%|████████▉ | 2.00G/2.24G [00:35<00:04, 48.2MB/s]Downloading pytorch_model.bin:  90%|████████▉ | 2.01G/2.24G [00:35<00:03, 57.2MB/s]Downloading pytorch_model.bin:  90%|█████████ | 2.02G/2.24G [00:35<00:03, 65.8MB/s]Downloading pytorch_model.bin:  91%|█████████ | 2.03G/2.24G [00:35<00:03, 66.4MB/s]Downloading pytorch_model.bin:  91%|█████████▏| 2.04G/2.24G [00:36<00:03, 63.5MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 2.06G/2.24G [00:36<00:03, 61.3MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 2.07G/2.24G [00:36<00:02, 60.0MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 2.08G/2.24G [00:36<00:02, 59.3MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 2.09G/2.24G [00:36<00:02, 58.8MB/s]Downloading pytorch_model.bin:  94%|█████████▎| 2.10G/2.24G [00:37<00:02, 58.3MB/s]Downloading pytorch_model.bin:  94%|█████████▍| 2.11G/2.24G [00:37<00:02, 57.8MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 2.12G/2.24G [00:37<00:02, 58.2MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 2.13G/2.24G [00:37<00:01, 58.0MB/s]Downloading pytorch_model.bin:  95%|█████████▌| 2.14G/2.24G [00:37<00:01, 57.9MB/s]Downloading pytorch_model.bin:  96%|█████████▌| 2.15G/2.24G [00:37<00:01, 57.8MB/s]Downloading pytorch_model.bin:  96%|█████████▋| 2.16G/2.24G [00:38<00:01, 57.3MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 2.17G/2.24G [00:38<00:01, 57.6MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 2.18G/2.24G [00:38<00:01, 57.6MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 2.19G/2.24G [00:38<00:00, 57.6MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 2.20G/2.24G [00:38<00:00, 57.6MB/s]Downloading pytorch_model.bin:  99%|█████████▊| 2.21G/2.24G [00:39<00:00, 57.6MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 2.22G/2.24G [00:39<00:00, 57.5MB/s]Downloading pytorch_model.bin: 100%|█████████▉| 2.23G/2.24G [00:39<00:00, 57.6MB/s]Downloading pytorch_model.bin: 100%|██████████| 2.24G/2.24G [00:39<00:00, 57.8MB/s]Downloading pytorch_model.bin: 100%|██████████| 2.24G/2.24G [00:39<00:00, 56.6MB/s]
Some weights of the model checkpoint at classla/xlm-r-bertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 43.7MB/s]Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 42.6MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 350kB/s]
Downloading tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|██████████| 399/399 [00:00<00:00, 549kB/s]
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training started. Current model: xlm-r-bertic, no. of epochs: 7
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 383.3 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.47566976426522944, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Evaluation completed.
It took 5.7 minutes for 51190 instances.
Run 0 finished.
Downloading config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 731/731 [00:00<00:00, 726kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|          | 10.5M/2.24G [00:00<00:52, 42.7MB/s]Downloading pytorch_model.bin:   1%|          | 21.0M/2.24G [00:00<01:20, 27.7MB/s]Downloading pytorch_model.bin:   1%|▏         | 31.5M/2.24G [00:00<00:53, 41.3MB/s]Downloading pytorch_model.bin:   2%|▏         | 41.9M/2.24G [00:01<01:22, 26.7MB/s]Downloading pytorch_model.bin:   2%|▏         | 52.4M/2.24G [00:01<01:02, 35.1MB/s]Downloading pytorch_model.bin:   3%|▎         | 62.9M/2.24G [00:01<00:48, 45.2MB/s]Downloading pytorch_model.bin:   3%|▎         | 73.4M/2.24G [00:01<00:39, 55.3MB/s]Downloading pytorch_model.bin:   4%|▎         | 83.9M/2.24G [00:01<00:36, 59.4MB/s]Downloading pytorch_model.bin:   4%|▍         | 94.4M/2.24G [00:02<00:31, 68.0MB/s]Downloading pytorch_model.bin:   5%|▍         | 105M/2.24G [00:02<00:45, 47.2MB/s] Downloading pytorch_model.bin:   5%|▌         | 115M/2.24G [00:02<00:37, 55.9MB/s]Downloading pytorch_model.bin:   6%|▌         | 126M/2.24G [00:02<00:32, 64.7MB/s]Downloading pytorch_model.bin:   6%|▌         | 136M/2.24G [00:02<00:28, 72.6MB/s]Downloading pytorch_model.bin:   7%|▋         | 147M/2.24G [00:03<00:42, 49.6MB/s]Downloading pytorch_model.bin:   7%|▋         | 157M/2.24G [00:03<01:03, 32.9MB/s]Downloading pytorch_model.bin:   7%|▋         | 168M/2.24G [00:03<00:50, 41.0MB/s]Downloading pytorch_model.bin:   8%|▊         | 178M/2.24G [00:03<00:41, 49.5MB/s]Downloading pytorch_model.bin:   8%|▊         | 189M/2.24G [00:03<00:35, 58.3MB/s]Downloading pytorch_model.bin:   9%|▉         | 199M/2.24G [00:04<00:30, 66.3MB/s]Downloading pytorch_model.bin:   9%|▉         | 210M/2.24G [00:04<00:28, 72.3MB/s]Downloading pytorch_model.bin:  10%|▉         | 220M/2.24G [00:04<00:25, 78.7MB/s]Downloading pytorch_model.bin:  10%|█         | 231M/2.24G [00:04<00:23, 84.1MB/s]Downloading pytorch_model.bin:  11%|█         | 241M/2.24G [00:04<00:22, 88.0MB/s]Downloading pytorch_model.bin:  11%|█         | 252M/2.24G [00:04<00:21, 92.3MB/s]Downloading pytorch_model.bin:  12%|█▏        | 262M/2.24G [00:04<00:21, 90.4MB/s]Downloading pytorch_model.bin:  12%|█▏        | 273M/2.24G [00:04<00:22, 85.9MB/s]Downloading pytorch_model.bin:  13%|█▎        | 283M/2.24G [00:05<00:26, 74.6MB/s]Downloading pytorch_model.bin:  13%|█▎        | 294M/2.24G [00:05<00:28, 68.3MB/s]Downloading pytorch_model.bin:  14%|█▎        | 304M/2.24G [00:05<00:29, 64.7MB/s]Downloading pytorch_model.bin:  14%|█▍        | 315M/2.24G [00:05<00:30, 62.2MB/s]Downloading pytorch_model.bin:  15%|█▍        | 325M/2.24G [00:06<00:53, 35.9MB/s]Downloading pytorch_model.bin:  15%|█▍        | 336M/2.24G [00:06<01:09, 27.3MB/s]Downloading pytorch_model.bin:  15%|█▌        | 346M/2.24G [00:06<00:54, 34.9MB/s]Downloading pytorch_model.bin:  16%|█▌        | 357M/2.24G [00:06<00:43, 43.4MB/s]Downloading pytorch_model.bin:  16%|█▋        | 367M/2.24G [00:07<00:36, 51.0MB/s]Downloading pytorch_model.bin:  17%|█▋        | 377M/2.24G [00:07<00:44, 41.8MB/s]Downloading pytorch_model.bin:  17%|█▋        | 388M/2.24G [00:07<00:37, 49.5MB/s]Downloading pytorch_model.bin:  18%|█▊        | 398M/2.24G [00:07<00:31, 57.9MB/s]Downloading pytorch_model.bin:  18%|█▊        | 409M/2.24G [00:07<00:29, 61.2MB/s]Downloading pytorch_model.bin:  19%|█▊        | 419M/2.24G [00:07<00:26, 68.7MB/s]Downloading pytorch_model.bin:  19%|█▉        | 430M/2.24G [00:08<00:24, 74.2MB/s]Downloading pytorch_model.bin:  20%|█▉        | 440M/2.24G [00:08<00:22, 80.0MB/s]Downloading pytorch_model.bin:  20%|██        | 451M/2.24G [00:08<00:22, 80.7MB/s]Downloading pytorch_model.bin:  21%|██        | 461M/2.24G [00:08<00:20, 85.1MB/s]Downloading pytorch_model.bin:  21%|██        | 472M/2.24G [00:08<00:19, 89.3MB/s]Downloading pytorch_model.bin:  22%|██▏       | 482M/2.24G [00:08<00:19, 90.9MB/s]Downloading pytorch_model.bin:  22%|██▏       | 493M/2.24G [00:08<00:30, 57.9MB/s]Downloading pytorch_model.bin:  22%|██▏       | 503M/2.24G [00:09<00:26, 64.4MB/s]Downloading pytorch_model.bin:  23%|██▎       | 514M/2.24G [00:09<00:24, 71.7MB/s]Downloading pytorch_model.bin:  23%|██▎       | 524M/2.24G [00:09<00:22, 76.7MB/s]Downloading pytorch_model.bin:  24%|██▍       | 535M/2.24G [00:09<00:25, 68.2MB/s]Downloading pytorch_model.bin:  24%|██▍       | 545M/2.24G [00:09<00:26, 64.5MB/s]Downloading pytorch_model.bin:  25%|██▍       | 556M/2.24G [00:09<00:27, 62.2MB/s]Downloading pytorch_model.bin:  25%|██▌       | 566M/2.24G [00:10<00:27, 60.6MB/s]Downloading pytorch_model.bin:  26%|██▌       | 577M/2.24G [00:10<00:36, 45.8MB/s]Downloading pytorch_model.bin:  26%|██▌       | 587M/2.24G [00:10<00:30, 55.0MB/s]Downloading pytorch_model.bin:  27%|██▋       | 598M/2.24G [00:10<00:25, 63.7MB/s]Downloading pytorch_model.bin:  27%|██▋       | 608M/2.24G [00:10<00:25, 63.4MB/s]Downloading pytorch_model.bin:  28%|██▊       | 619M/2.24G [00:10<00:26, 61.0MB/s]Downloading pytorch_model.bin:  28%|██▊       | 629M/2.24G [00:11<00:26, 59.8MB/s]Downloading pytorch_model.bin:  29%|██▊       | 640M/2.24G [00:11<00:27, 58.9MB/s]Downloading pytorch_model.bin:  29%|██▉       | 650M/2.24G [00:11<00:27, 58.2MB/s]Downloading pytorch_model.bin:  29%|██▉       | 661M/2.24G [00:11<00:27, 57.8MB/s]Downloading pytorch_model.bin:  30%|██▉       | 671M/2.24G [00:11<00:27, 57.6MB/s]Downloading pytorch_model.bin:  30%|███       | 682M/2.24G [00:12<00:27, 57.5MB/s]Downloading pytorch_model.bin:  31%|███       | 692M/2.24G [00:12<00:27, 57.4MB/s]Downloading pytorch_model.bin:  31%|███▏      | 703M/2.24G [00:12<00:27, 56.4MB/s]Downloading pytorch_model.bin:  32%|███▏      | 713M/2.24G [00:12<00:26, 56.8MB/s]Downloading pytorch_model.bin:  32%|███▏      | 724M/2.24G [00:12<00:26, 56.9MB/s]Downloading pytorch_model.bin:  33%|███▎      | 734M/2.24G [00:13<00:27, 55.2MB/s]Downloading pytorch_model.bin:  33%|███▎      | 744M/2.24G [00:13<00:26, 57.5MB/s]Downloading pytorch_model.bin:  34%|███▎      | 755M/2.24G [00:13<00:25, 57.2MB/s]Downloading pytorch_model.bin:  34%|███▍      | 765M/2.24G [00:13<00:25, 57.2MB/s]Downloading pytorch_model.bin:  35%|███▍      | 776M/2.24G [00:13<00:25, 58.1MB/s]Downloading pytorch_model.bin:  35%|███▌      | 786M/2.24G [00:13<00:25, 57.8MB/s]Downloading pytorch_model.bin:  36%|███▌      | 797M/2.24G [00:14<00:25, 55.7MB/s]Downloading pytorch_model.bin:  36%|███▌      | 807M/2.24G [00:14<00:25, 55.6MB/s]Downloading pytorch_model.bin:  37%|███▋      | 818M/2.24G [00:14<00:31, 45.5MB/s]Downloading pytorch_model.bin:  37%|███▋      | 828M/2.24G [00:14<00:26, 54.2MB/s]Downloading pytorch_model.bin:  37%|███▋      | 839M/2.24G [00:14<00:22, 61.5MB/s]Downloading pytorch_model.bin:  38%|███▊      | 849M/2.24G [00:15<00:23, 58.4MB/s]Downloading pytorch_model.bin:  38%|███▊      | 860M/2.24G [00:15<00:23, 59.8MB/s]Downloading pytorch_model.bin:  39%|███▉      | 870M/2.24G [00:15<00:23, 59.0MB/s]Downloading pytorch_model.bin:  39%|███▉      | 881M/2.24G [00:15<00:23, 58.4MB/s]Downloading pytorch_model.bin:  40%|███▉      | 891M/2.24G [00:15<00:23, 58.1MB/s]Downloading pytorch_model.bin:  40%|████      | 902M/2.24G [00:15<00:23, 57.8MB/s]Downloading pytorch_model.bin:  41%|████      | 912M/2.24G [00:16<00:23, 57.6MB/s]Downloading pytorch_model.bin:  41%|████      | 923M/2.24G [00:16<00:23, 56.8MB/s]Downloading pytorch_model.bin:  42%|████▏     | 933M/2.24G [00:16<00:22, 57.6MB/s]Downloading pytorch_model.bin:  42%|████▏     | 944M/2.24G [00:16<00:22, 56.7MB/s]Downloading pytorch_model.bin:  43%|████▎     | 954M/2.24G [00:17<00:28, 44.9MB/s]Downloading pytorch_model.bin:  43%|████▎     | 965M/2.24G [00:17<00:24, 53.0MB/s]Downloading pytorch_model.bin:  44%|████▎     | 975M/2.24G [00:17<00:20, 60.7MB/s]Downloading pytorch_model.bin:  44%|████▍     | 986M/2.24G [00:17<00:20, 62.3MB/s]Downloading pytorch_model.bin:  44%|████▍     | 996M/2.24G [00:17<00:20, 60.7MB/s]Downloading pytorch_model.bin:  45%|████▍     | 1.01G/2.24G [00:17<00:20, 59.1MB/s]Downloading pytorch_model.bin:  45%|████▌     | 1.02G/2.24G [00:17<00:20, 58.4MB/s]Downloading pytorch_model.bin:  46%|████▌     | 1.03G/2.24G [00:18<00:27, 44.5MB/s]Downloading pytorch_model.bin:  46%|████▋     | 1.04G/2.24G [00:18<00:30, 39.4MB/s]Downloading pytorch_model.bin:  47%|████▋     | 1.05G/2.24G [00:18<00:24, 48.0MB/s]Downloading pytorch_model.bin:  47%|████▋     | 1.06G/2.24G [00:18<00:20, 57.0MB/s]Downloading pytorch_model.bin:  48%|████▊     | 1.07G/2.24G [00:19<00:17, 65.4MB/s]Downloading pytorch_model.bin:  48%|████▊     | 1.08G/2.24G [00:19<00:24, 46.6MB/s]Downloading pytorch_model.bin:  49%|████▊     | 1.09G/2.24G [00:19<00:20, 55.8MB/s]Downloading pytorch_model.bin:  49%|████▉     | 1.10G/2.24G [00:19<00:17, 64.6MB/s]Downloading pytorch_model.bin:  50%|████▉     | 1.11G/2.24G [00:19<00:15, 72.0MB/s]Downloading pytorch_model.bin:  50%|█████     | 1.12G/2.24G [00:19<00:14, 74.8MB/s]Downloading pytorch_model.bin:  51%|█████     | 1.13G/2.24G [00:19<00:16, 68.4MB/s]Downloading pytorch_model.bin:  51%|█████     | 1.14G/2.24G [00:20<00:17, 64.6MB/s]Downloading pytorch_model.bin:  51%|█████▏    | 1.15G/2.24G [00:20<00:24, 45.3MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 1.16G/2.24G [00:20<00:19, 54.0MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 1.17G/2.24G [00:20<00:17, 62.3MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 1.18G/2.24G [00:20<00:15, 67.2MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 1.20G/2.24G [00:21<00:16, 62.9MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 1.21G/2.24G [00:21<00:17, 60.3MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 1.22G/2.24G [00:21<00:17, 59.3MB/s]Downloading pytorch_model.bin:  55%|█████▍    | 1.23G/2.24G [00:21<00:17, 58.1MB/s]Downloading pytorch_model.bin:  55%|█████▌    | 1.24G/2.24G [00:21<00:17, 57.6MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 1.25G/2.24G [00:22<00:17, 56.9MB/s]Downloading pytorch_model.bin:  56%|█████▌    | 1.26G/2.24G [00:22<00:17, 57.1MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 1.27G/2.24G [00:22<00:16, 57.2MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 1.28G/2.24G [00:22<00:16, 57.1MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.29G/2.24G [00:22<00:16, 57.2MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.30G/2.24G [00:22<00:16, 57.1MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 1.31G/2.24G [00:23<00:16, 57.1MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 1.32G/2.24G [00:23<00:16, 57.2MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 1.33G/2.24G [00:23<00:15, 57.1MB/s]Downloading pytorch_model.bin:  60%|█████▉    | 1.34G/2.24G [00:23<00:15, 57.2MB/s]Downloading pytorch_model.bin:  60%|██████    | 1.35G/2.24G [00:23<00:15, 56.7MB/s]Downloading pytorch_model.bin:  61%|██████    | 1.36G/2.24G [00:24<00:15, 56.7MB/s]Downloading pytorch_model.bin:  61%|██████▏   | 1.37G/2.24G [00:24<00:15, 56.7MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 1.38G/2.24G [00:24<00:15, 55.9MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 1.39G/2.24G [00:24<00:14, 56.5MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 1.41G/2.24G [00:24<00:14, 56.9MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 1.42G/2.24G [00:24<00:14, 56.8MB/s]Downloading pytorch_model.bin:  64%|██████▎   | 1.43G/2.24G [00:25<00:14, 56.9MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 1.44G/2.24G [00:25<00:14, 54.4MB/s]Downloading pytorch_model.bin:  65%|██████▍   | 1.45G/2.24G [00:25<00:13, 57.6MB/s]Downloading pytorch_model.bin:  65%|██████▌   | 1.46G/2.24G [00:25<00:13, 57.4MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 1.47G/2.24G [00:25<00:13, 57.4MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 1.48G/2.24G [00:26<00:13, 56.6MB/s]Downloading pytorch_model.bin:  66%|██████▋   | 1.49G/2.24G [00:26<00:13, 56.9MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 1.50G/2.24G [00:26<00:13, 56.9MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 1.51G/2.24G [00:26<00:12, 57.1MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 1.52G/2.24G [00:26<00:12, 57.1MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 1.53G/2.24G [00:27<00:12, 57.2MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 1.54G/2.24G [00:27<00:12, 56.9MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 1.55G/2.24G [00:27<00:12, 56.7MB/s]Downloading pytorch_model.bin:  70%|██████▉   | 1.56G/2.24G [00:27<00:11, 56.9MB/s]Downloading pytorch_model.bin:  70%|███████   | 1.57G/2.24G [00:27<00:11, 56.8MB/s]Downloading pytorch_model.bin:  71%|███████   | 1.58G/2.24G [00:27<00:11, 56.7MB/s]Downloading pytorch_model.bin:  71%|███████   | 1.59G/2.24G [00:28<00:11, 56.7MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.60G/2.24G [00:28<00:11, 56.6MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 1.61G/2.24G [00:28<00:11, 56.4MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.63G/2.24G [00:28<00:10, 56.4MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.64G/2.24G [00:28<00:10, 56.6MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 1.65G/2.24G [00:29<00:10, 56.8MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 1.66G/2.24G [00:29<00:10, 56.8MB/s]Downloading pytorch_model.bin:  74%|███████▍  | 1.67G/2.24G [00:29<00:10, 56.9MB/s]Downloading pytorch_model.bin:  75%|███████▍  | 1.68G/2.24G [00:29<00:09, 57.0MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 1.69G/2.24G [00:29<00:09, 56.4MB/s]Downloading pytorch_model.bin:  76%|███████▌  | 1.70G/2.24G [00:30<00:10, 53.7MB/s]Downloading pytorch_model.bin:  76%|███████▋  | 1.71G/2.24G [00:30<00:09, 58.3MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.72G/2.24G [00:30<00:09, 57.9MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 1.73G/2.24G [00:30<00:08, 57.8MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 1.74G/2.24G [00:30<00:10, 46.4MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 1.75G/2.24G [00:31<00:12, 38.6MB/s]Downloading pytorch_model.bin:  79%|███████▊  | 1.76G/2.24G [00:31<00:10, 46.7MB/s]Downloading pytorch_model.bin:  79%|███████▉  | 1.77G/2.24G [00:31<00:12, 37.5MB/s]Downloading pytorch_model.bin:  80%|███████▉  | 1.78G/2.24G [00:31<00:10, 44.1MB/s]Downloading pytorch_model.bin:  80%|████████  | 1.79G/2.24G [00:32<00:08, 52.1MB/s]Downloading pytorch_model.bin:  80%|████████  | 1.80G/2.24G [00:32<00:07, 60.3MB/s]Downloading pytorch_model.bin:  81%|████████  | 1.81G/2.24G [00:32<00:06, 62.8MB/s]Downloading pytorch_model.bin:  81%|████████▏ | 1.82G/2.24G [00:32<00:06, 67.2MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 1.84G/2.24G [00:32<00:05, 74.9MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 1.85G/2.24G [00:32<00:07, 50.1MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 1.86G/2.24G [00:32<00:06, 59.3MB/s]Downloading pytorch_model.bin:  83%|████████▎ | 1.87G/2.24G [00:33<00:05, 66.1MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 1.88G/2.24G [00:33<00:05, 70.6MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 1.89G/2.24G [00:33<00:04, 76.7MB/s]Downloading pytorch_model.bin:  85%|████████▍ | 1.90G/2.24G [00:33<00:04, 72.9MB/s]Downloading pytorch_model.bin:  85%|████████▌ | 1.91G/2.24G [00:33<00:06, 49.6MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 1.92G/2.24G [00:33<00:05, 57.4MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 1.93G/2.24G [00:34<00:04, 64.8MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 1.94G/2.24G [00:34<00:04, 68.2MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 1.95G/2.24G [00:34<00:04, 63.3MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 1.96G/2.24G [00:34<00:04, 61.6MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 1.97G/2.24G [00:34<00:04, 60.1MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 1.98G/2.24G [00:34<00:04, 58.8MB/s]Downloading pytorch_model.bin:  89%|████████▉ | 1.99G/2.24G [00:35<00:04, 57.9MB/s]Downloading pytorch_model.bin:  89%|████████▉ | 2.00G/2.24G [00:35<00:04, 57.7MB/s]Downloading pytorch_model.bin:  90%|████████▉ | 2.01G/2.24G [00:35<00:03, 57.6MB/s]Downloading pytorch_model.bin:  90%|█████████ | 2.02G/2.24G [00:35<00:03, 57.0MB/s]Downloading pytorch_model.bin:  91%|█████████ | 2.03G/2.24G [00:35<00:03, 54.2MB/s]Downloading pytorch_model.bin:  91%|█████████▏| 2.04G/2.24G [00:36<00:03, 53.4MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 2.06G/2.24G [00:36<00:03, 56.6MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 2.07G/2.24G [00:36<00:02, 58.4MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 2.08G/2.24G [00:36<00:02, 58.2MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 2.09G/2.24G [00:36<00:02, 58.0MB/s]Downloading pytorch_model.bin:  94%|█████████▎| 2.10G/2.24G [00:37<00:02, 57.7MB/s]Downloading pytorch_model.bin:  94%|█████████▍| 2.11G/2.24G [00:37<00:02, 57.6MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 2.12G/2.24G [00:37<00:02, 57.3MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 2.13G/2.24G [00:37<00:01, 57.0MB/s]Downloading pytorch_model.bin:  95%|█████████▌| 2.14G/2.24G [00:37<00:01, 57.0MB/s]Downloading pytorch_model.bin:  96%|█████████▌| 2.15G/2.24G [00:37<00:01, 56.9MB/s]Downloading pytorch_model.bin:  96%|█████████▋| 2.16G/2.24G [00:38<00:01, 57.1MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 2.17G/2.24G [00:38<00:01, 56.3MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 2.18G/2.24G [00:38<00:01, 56.4MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 2.19G/2.24G [00:38<00:00, 56.8MB/s]Downloading pytorch_model.bin:  98%|█████████▊| 2.20G/2.24G [00:38<00:00, 56.7MB/s]Downloading pytorch_model.bin:  99%|█████████▊| 2.21G/2.24G [00:39<00:00, 56.9MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 2.22G/2.24G [00:39<00:00, 56.8MB/s]Downloading pytorch_model.bin: 100%|█████████▉| 2.23G/2.24G [00:39<00:00, 57.2MB/s]Downloading pytorch_model.bin: 100%|██████████| 2.24G/2.24G [00:39<00:00, 57.0MB/s]Downloading pytorch_model.bin: 100%|██████████| 2.24G/2.24G [00:39<00:00, 56.7MB/s]
Some weights of the model checkpoint at classla/xlm-r-slobertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-slobertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 9.32MB/s]Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 9.23MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 263kB/s]
Downloading tokenizer_config.json:   0%|          | 0.00/416 [00:00<?, ?B/s]Downloading tokenizer_config.json: 100%|██████████| 416/416 [00:00<00:00, 296kB/s]
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training started. Current model: xlm-r-slobertic, no. of epochs: 7
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 383.33 minutes for 398681 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.40104700032761925, 'precision': 0.34658329611433675, 'recall': 0.18203143326296037, 'f1_score': 0.23869578591202706}
Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 5.05 minutes for 51190 instances.
Run 0 finished.
['B-per', 'O', 'B-org', 'B-loc', 'I-org', 'B-misc', 'I-misc', 'I-loc', 'B-deriv-per', 'I-per', 'I-deriv-per']
(71967, 4) (8952, 4) (8936, 4)
   sentence_id    words labels  id
0            0   Vakula  B-per   0
1            1    dragi      O   1
2            2  Drakula  B-per   2
3            3        ,      O   3
4            4     kiša      O   4
Training started. Current model: csebert, no. of epochs: 7
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of bert model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 23.37 minutes for 71967 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.29489891301637466, 'precision': 0.7256637168141593, 'recall': 0.6693877551020408, 'f1_score': 0.6963906581740977}
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.58 minutes for 8952 instances.
Run 0 finished.
Training started. Current model: xlm-r-base, no. of epochs: 8
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 30.61 minutes for 71967 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.273053216231093, 'precision': 0.7348242811501597, 'recall': 0.6258503401360545, 'f1_score': 0.6759735488611316}
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.54 minutes for 8952 instances.
Run 0 finished.
Training started. Current model: xlm-r-large, no. of epochs: 11
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 109.13 minutes for 71967 instances.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.4367366753272226, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']
- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.79 minutes for 8952 instances.
Run 0 finished.
Training started. Current model: bertic, no. of epochs: 10
INFO:simpletransformers.ner.ner_model: Training of electra model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 32.71 minutes for 71967 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.2499152744343709, 'precision': 0.7579908675799086, 'recall': 0.6775510204081633, 'f1_score': 0.7155172413793104}
Some weights of the model checkpoint at classla/xlm-r-bertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.53 minutes for 8952 instances.
Run 0 finished.
Training started. Current model: xlm-r-bertic, no. of epochs: 11
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 108.69 minutes for 71967 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.45603353110033024, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Some weights of the model checkpoint at classla/xlm-r-slobertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-slobertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.85 minutes for 8952 instances.
Run 0 finished.
Training started. Current model: xlm-r-slobertic, no. of epochs: 11
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 109.12 minutes for 71967 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.473178324899682, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.83 minutes for 8952 instances.
Run 0 finished.
['B-per', 'O', 'B-deriv-per', 'B-misc', 'I-misc', 'I-per', 'B-org', 'I-org', 'B-loc', 'I-loc', 'B-O', 'I-O']
(73943, 4) (9122, 4) (9206, 4)
   sentence_id      words labels  id
0            0  @vukomand  B-per   0
1            1    Gospođo      O   1
2            2     Dijana  B-per   2
3            3       koje      O   3
4            4     lekove      O   4
Training started. Current model: csebert, no. of epochs: 7
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of bert model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 23.98 minutes for 73943 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.19591299932218245, 'precision': 0.739240506329114, 'recall': 0.6334056399132321, 'f1_score': 0.6822429906542056}
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.59 minutes for 9122 instances.
Run 0 finished.
Training started. Current model: xlm-r-base, no. of epochs: 8
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 31.55 minutes for 73943 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.18144320099109312, 'precision': 0.7493188010899182, 'recall': 0.596529284164859, 'f1_score': 0.6642512077294686}
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.56 minutes for 9122 instances.
Run 0 finished.
Training started. Current model: xlm-r-large, no. of epochs: 11
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 111.62 minutes for 73943 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.24688449227736772, 'precision': 0.7931034482758621, 'recall': 0.2993492407809111, 'f1_score': 0.4346456692913386}
Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']
- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.81 minutes for 9122 instances.
Run 0 finished.
Training started. Current model: bertic, no. of epochs: 10
INFO:simpletransformers.ner.ner_model: Training of electra model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 33.59 minutes for 73943 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.17747151424600616, 'precision': 0.7573529411764706, 'recall': 0.6702819956616052, 'f1_score': 0.7111622554660528}
Some weights of the model checkpoint at classla/xlm-r-bertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.55 minutes for 9122 instances.
Run 0 finished.
Training started. Current model: xlm-r-bertic, no. of epochs: 11
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 112.2 minutes for 73943 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.18654980772321, 'precision': 0.75, 'recall': 0.6442516268980477, 'f1_score': 0.6931155192532088}
Some weights of the model checkpoint at classla/xlm-r-slobertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-slobertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.85 minutes for 9122 instances.
Run 0 finished.
Training started. Current model: xlm-r-slobertic, no. of epochs: 11
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 111.81 minutes for 73943 instances.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.30809409486735523, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Some weights of the model checkpoint at EMBEDDIA/crosloengual-bert were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at EMBEDDIA/crosloengual-bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.83 minutes for 9122 instances.
Run 0 finished.
['O', 'B-loc', 'B-org', 'B-per', 'I-per', 'B-deriv-per', 'I-org', 'I-loc', 'B-misc', 'I-misc']
(74259, 4) (11421, 4) (11993, 4)
     sentence_id      words labels   id
726          726      Kazna      O  726
727          727  medijskom      O  727
728          728     mogulu      O  728
729          729   obnovila      O  729
730          730     debatu      O  730
Training started. Current model: csebert, no. of epochs: 9
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
INFO:simpletransformers.ner.ner_model: Training of bert model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 30.79 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.2578192421889945, 'precision': 0.8236245954692557, 'recall': 0.7512915129151292, 'f1_score': 0.785796989579313}
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.67 minutes for 11421 instances.
Run 0 finished.
Training started. Current model: xlm-r-base, no. of epochs: 6
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 23.92 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.21483951077198707, 'precision': 0.8268434134217068, 'recall': 0.7365313653136532, 'f1_score': 0.7790788446526152}
Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.68 minutes for 11421 instances.
Run 0 finished.
Training started. Current model: xlm-r-large, no. of epochs: 13
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 132.42 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.3179568883709891, 'precision': 0.8257956448911222, 'recall': 0.7276752767527676, 'f1_score': 0.7736367202824638}
Some weights of the model checkpoint at classla/bcms-bertic were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']
- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at classla/bcms-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 1.0 minutes for 11421 instances.
Run 0 finished.
Training started. Current model: bertic, no. of epochs: 10
INFO:simpletransformers.ner.ner_model: Training of electra model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 33.88 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.21540227549291663, 'precision': 0.8428686543110395, 'recall': 0.7719557195571956, 'f1_score': 0.8058551617873652}
Some weights of the model checkpoint at classla/xlm-r-bertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-bertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 0.68 minutes for 11421 instances.
Run 0 finished.
Training started. Current model: xlm-r-bertic, no. of epochs: 13
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 132.75 minutes for 74259 instances.
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.3038863734809103, 'precision': 0.8316831683168316, 'recall': 0.7439114391143912, 'f1_score': 0.7853525516166732}
Some weights of the model checkpoint at classla/xlm-r-slobertic were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at classla/xlm-r-slobertic and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Evaluation completed.
It took 1.05 minutes for 11421 instances.
Run 0 finished.
Training started. Current model: xlm-r-slobertic, no. of epochs: 13
INFO:simpletransformers.ner.ner_model: Training of xlmroberta model complete. Saved to outputs/.
INFO:simpletransformers.ner.ner_model: Converting to features started.
Training completed.
It took 132.49 minutes for 74259 instances.
/home/tajak/NER-recognition/ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
INFO:simpletransformers.ner.ner_model:{'eval_loss': 0.611716150235729, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}
Evaluation completed.
It took 1.01 minutes for 11421 instances.
Run 0 finished.
